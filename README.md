# MultiModal AI Dataset Creator

A comprehensive toolkit for creating professional-grade ML training datasets with annotations for multiple modalities including image classification, object detection, text sentiment analysis, NER, and audio transcription.

## ğŸ¯ Project Overview

This project demonstrates the complete workflow of creating a multi-modal ML dataset from scratch using industry-standard annotation tools and best practices. The project includes:

- **Multi-modal data collection** (Images, Text, Audio)
- **Professional annotation workflows** using tools like LabelImg, Doccano, CVAT, and Label Studio
- **Quality assurance practices** with tracking and validation
- **Format conversions** (COCO â†” YOLO â†” CSV)
- **Comprehensive documentation** and annotation guidelines
- **Interactive dashboard** built with Streamlit

## ğŸ› ï¸ Tools Used

### Annotation Tools
- **Label Studio**: Image classification, Intent classification
- **LabelImg**: Object detection (bounding boxes)
- **CVAT**: Semantic segmentation
- **Doccano**: Sentiment analysis, NER
- **Audacity**: Audio transcription (manual)

### Development Stack
- **Python**: Data processing and format conversion
- **Streamlit**: Interactive dashboard
- **Excel/CSV**: QA tracking and reporting
- **Git**: Version control

## ğŸ“ Project Structure

```
MultiModal-AI-Dataset-Creator/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ images/          # Raw image files
â”‚   â”œâ”€â”€ text/           # Text samples (reviews, comments)
â”‚   â””â”€â”€ audio/          # Audio files (WAV format)
â”‚
â”œâ”€â”€ annotations/
â”‚   â”œâ”€â”€ image_labels.json    # Image annotations (COCO format)
â”‚   â”œâ”€â”€ text_labels.csv      # Text annotations
â”‚   â””â”€â”€ audio_labels.csv     # Audio transcriptions
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ format_converter.py  # Convert between annotation formats
â”‚   â”œâ”€â”€ qa_checker.py        # Quality assurance validation
â”‚   â””â”€â”€ file_renamer.py      # Systematic file naming
â”‚
â”œâ”€â”€ qa/
â”‚   â”œâ”€â”€ qa_log.csv          # Quality assurance tracking
â”‚   â””â”€â”€ label_agreement.csv  # Inter-annotator agreement
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ annotation_guidelines.md  # Annotation standards
â”‚   â”œâ”€â”€ edge_cases.md             # Edge case documentation
â”‚   â””â”€â”€ summary_report.pdf        # Final project report
â”‚
â”œâ”€â”€ streamlit_app/
â”‚   â””â”€â”€ app.py              # Interactive dashboard
â”‚
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md              # This file
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- pip package manager
- Git

### Installation

1. Clone the repository:
```bash
git clone <your-repo-url>
cd MultiModal-AI-Dataset-Creator
```

2. Install Python dependencies:
```bash
pip install -r requirements.txt
```

3. Launch the Streamlit dashboard:
```bash
streamlit run streamlit_app/app.py
```

## ğŸ“Š Dataset Types

### ğŸ–¼ï¸ Image Data
- **Classification**: Fruits, tools, signboards (50-100 images)
- **Object Detection**: Bounding box annotations
- **Segmentation**: Pixel-level annotations
- **Formats**: COCO JSON, YOLO TXT, CSV

### ğŸ“„ Text Data
- **Sentiment Analysis**: Product reviews, social media comments
- **NER**: Named entity recognition
- **Intent Classification**: User queries and intents
- **Formats**: JSON, CSV, CoNLL

### ğŸ”Š Audio Data
- **Transcription**: Speech-to-text
- **Speaker Diarization**: Speaker identification
- **Formats**: WAV, CSV annotations

## ğŸ” Quality Assurance

- **Validation Scripts**: Automated consistency checking
- **QA Tracking**: Excel-based progress monitoring
- **Gold Standard**: Reference annotations for validation
- **Edge Case Documentation**: Handling of ambiguous cases

## ğŸ“ˆ Usage Examples

### Format Conversion
```python
from scripts.format_converter import convert_coco_to_yolo
convert_coco_to_yolo('annotations/image_labels.json', 'output/')
```

### Quality Check
```python
from scripts.qa_checker import validate_annotations
validate_annotations('annotations/', 'qa/qa_log.csv')
```

## ğŸ¤ Contributing

1. Follow the annotation guidelines in `docs/annotation_guidelines.md`
2. Use the QA checklist for validation
3. Document edge cases and decisions
4. Update the QA log for all changes

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ“ Contact

For questions about annotation guidelines or dataset usage, please refer to the documentation in the `docs/` folder or create an issue in this repository.
Linkedin : https://www.linkedin.com/in/krishna-siddamsetti-045862290/
mail: krishnasiddamsetti15@gmail.com

